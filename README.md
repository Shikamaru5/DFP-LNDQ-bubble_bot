# LNDQ-bubble_bot
This is my attempt to try and speed up progress on training time for reinforcement learning. It's the culmination of countless hours of testing and tweaking, to get the algorithm to play the game bubble bobble. https://www.retrogames.cz/play_216-NES.php?language=EN this is the link to the website I'm using. I have not yet finished testing and attempting to get this to be significantly faster for training but I will not stop until I do.
I'd been working on changing the basic SAC or soft-actor critic deep-q reinforcement learning algorithm that was taught by the YouTuber Sentdex. I found that for a game such as Bubble Bobble some of the techniques used were insufficient for my purposes. So I've changed things like it doesn't use random samples of a minibatch of states. The algorithm will provide as a batch the states it's collected up to 64 states to update. I use PyTesseract in order to do OCR or Optical Character Recognition, this allows the model to read the score right from the screen which is a big part of how I've been training the model. I don't want to plug it into the game and give it data from the games systems, it feels like a step in the wrong direction to do that. By all means though don't let me stop you. I'll provide the Ocrtest script I use to make sure that my screen is aligned w/ the algorithms cameras whenever I start a new session.
Recently a paper came out and the subsequent code describing using reinforcement learning to teach a robot dog to walk in 20-30 minutes. This intrigued me so there are techniques that they've used in the program as well. They main take away from that research seems to be w/ a large amount of updates to train the model, and w/ a large amount of target_model q-predictions, that are then layer normalized it should reduce training time.
From what I can tell thus far, it has increased the performance of the model @ least twenty percent, however, it isn't as dramtic as the paper claims. Also, I had to make the model only update after each episode is complete so that the 220 updates it makes don't interfere w/ the stepping through the environment.
The github page is ikostrikov/walk_in_the_park if you wish to review their work.

